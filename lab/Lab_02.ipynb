{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\" style=\"border-style: hidden\" class=\"table\"> <tr><td class=\"col-md-2\"><img style=\"float\" src=\"http://prob140.org/assets/icon256.png\" alt=\"Prob140 Logo\" style=\"width: 120px;\"/></td><td><div align=\"left\"><h3 style=\"margin-top: 0;\">Probability for Data Science</h3><h4 style=\"margin-top: 20px;\">UC Berkeley, Fall 2020</h4><p>Ani Adhikari and Jim Pitman</p>CC BY-NC 4.0</div></td></tr></table><!-- not in pdf -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "from prob140 import *\n",
    "\n",
    "# These lines do some fancy plotting magic\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Useful for probability calculations\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Lab Resources\n",
    "\n",
    "* [`prob 140` Library Documentation](http://prob140.org/prob140/)\n",
    "* [`datascience` Library Documentation](http://data8.org/datascience/)\n",
    "* [Prob 140 Code Reference Sheet](http://prob140.org/assets/prob140_code_reference.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "# Lab 2: Total Variation #\n",
    "\n",
    "In Data 8, you measured the difference between two categorical distributions by calculating the total variation distance between them. In this lab, you will start by interpreting the total variation distance (TVD) in terms of probabilities. You will then use total variation distance to measure the difference between two probability distributions on the non-negative integers. \n",
    "\n",
    "This will give you a way to quantify how well one distribution approximates another. \n",
    "\n",
    "The focus will be on [Poisson distributions](http://prob140.org/textbook/content/Chapter_07/00_Poissonization.html), which are often used as approximations to distributions of counts of rare events. In particular, the Poisson $(1)$ distribution approximates the distributions of some random counts that have $1$ as their most likely value.\n",
    "\n",
    "A random variable $X$ has the Poisson $(1)$ distribution if\n",
    "\n",
    "$$\n",
    "P(X = k) ~ = ~ e^{-1} \\frac{1}{k!}, ~~~ k \\ge 0\n",
    "$$\n",
    "\n",
    "This is a probability distribution on infinitely many possible values. We will need that infinite support if we are going to approximate distributions on the values $0, 1, 2, \\ldots, n$ for arbitrarily large $n$.\n",
    "\n",
    "In class we are studying a situation in which probabilities approach those in a Poisson distribution: counting the number of successes in $n$ i.i.d. Bernoulli $(p)$ trials, for large $n$ and small $p$.\n",
    "\n",
    "In this lab you will look at the binomial $(n, 1/n)$ distributions and compare them with their Poisson $(1)$ approximations, both visually and also by a numerical measure of the distance between two distributions. In doing so, you will find an upper bound for the amount of error when you use the approximations instead of the exact probabilities.\n",
    "\n",
    "What you will learn:\n",
    "- The Data 8 definition of total variation distance (TVD) and its interpretation in terms of the amount of error in approximating probabilities\n",
    "- For large $n$, properties of the TVD between the binomial $(n, 1/n)$ and Poisson $(1)$ distributions\n",
    "- The proof of why the Data 8 definition of the TVD measures the worst error you can make in using one distribution to approximate another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Instructions\n",
    "Your labs have two components: a written portion and a portion that also involves code. Written work should be completed on paper, and coding questions should be done in the notebook. You are welcome to LaTeX your answers to the written portions, but staff will not be able to assist you with LaTeX related issues. It is your responsibility to ensure that both components of the lab are submitted completely and properly to Gradescope. Refer to the bottom of the notebook for submission instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Part 1: Total Variation Distance ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Suppose you have two probability distributions on the same set of possible values $x_1, x_2, \\ldots , x_n$. Let the two distributions be $b_1, b_2, \\ldots, b_n$ and $g_1, g_2, \\ldots, g_n$, where for each $i$ the $b$-distribution assigns probability $b_i$ to the value $x_i$ and the $g$-distribution assigns probability $g_i$.\n",
    "\n",
    "The *total variation distance* between the two distributions is defined by\n",
    "\n",
    "$$\n",
    "tvd(b, g) = \n",
    "\\frac{1}{2} \\sum_{i=1}^n |b_i - g_i| \n",
    "$$\n",
    "\n",
    "The choice of notation comes from the blue and gold colors you will see in overlaid histograms below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 1a) Computing TVD ###\n",
    "Define a function `tvd` that takes two probability arrays as arguments and returns the total variation distance between them. You should just assume that both of the input arrays will be probability distributions. You don't have to include code to check that each array is non-negative and sums to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "def tvd(b, g):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "When the two arrays are $b = [0.4, 0.3, 0.2, 0.1]$ and $g = [0.25, 0.35, 0.25, 0.15]$, the histograms look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "outputs": [],
   "source": [
    "b = make_array(0.4, 0.3, 0.2, 0.1)\n",
    "g = make_array(0.25, 0.35, 0.25, 0.15)\n",
    "\n",
    "k = np.arange(4)\n",
    "\n",
    "blue_dist = Table().values(k).probabilities(b)\n",
    "gold_dist = Table().values(k).probabilities(g)\n",
    "\n",
    "Plots('Distribution 1', blue_dist, 'Distribution 2', gold_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Calculate the TVD by mental math. Then run the cell below to confirm that your function `tvd` is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "tvd(b, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "The total variation distance between the two distributions is the total amount by which the areas of the blue bars exceed those of the corresponding gold bars. That's exactly equal to the total amount by which the gold bars exceed the blue.\n",
    "\n",
    "This is almost apparent from the definition of total variation distance, and you will prove it later in this lab. Just assume it for now as you did in Data 8. It is an intuitively reasonable measure of the difference between the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 1b) Another Way of Interpreting TVD ###\n",
    "\n",
    "**This part will be done in section on Wednesday; don't include it in your Gradescope submission.**\n",
    "\n",
    "Thus far, our interpretation of total variation distance has been essentially geometric: the amount by which the blue bars exceed the gold. There is an equivalent interpretation in terms of probabilities that makes it easier to understand what the numerical value of the distance is telling us.\n",
    "\n",
    "Suppose you have a finite set of possible values, and a choice of two probability distributions to use for finding probabilities. For example, the choices might be the exact distribution of a random variable and an approximate distribution. \n",
    "\n",
    "**The total variation distance between the two distributions is the biggest difference you can possibly get if you compute the probability of an event using each of the two distributions.**\n",
    "\n",
    "Formally, if $S$ is the space of all possible values, then the total variation distance between the blue and gold distributions is equal to\n",
    "\n",
    "$$\n",
    "\\max\\{ \\big{\\lvert} P_{blue}(A) - P_{gold}(A) \\big{\\rvert} : A \\subseteq S\\}\n",
    "$$\n",
    "\n",
    "This can be shown in a few straightforward steps, which you will do later in this lab. For now, confirm that it is true for the distributions in **1a**, in the following steps.\n",
    "\n",
    "- Figure out how many events can be created out of the outcomes $\\{0, 1, 2, 3\\}$.\n",
    "- List all the events.\n",
    "- For each event, compute the absolute difference between the blue and gold probabilities of the event. Your goal is to find the biggest possible absolute difference, so you might not even need to compute each one.\n",
    "- See which event or events correspond to the biggest absolute difference, and compare the value of that absolute difference with the TVD that you computed in **1a**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Part 2. Poisson $(1)$ Approximation to Binomial Distributions ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Recall that $X$ has the Poisson distribution with parameter 1 if \n",
    "\n",
    "$$\n",
    "P(X = k) ~ = ~ e^{-1} \\frac{1}{k!}, ~~~ k \\ge 0\n",
    "$$\n",
    "\n",
    "The `stats` module of SciPy can be used to calculate the probabilities in this distribution. If `values` is an array of non-negative integers, then\n",
    "\n",
    "`stats.poisson.pmf(values, 1)`\n",
    "\n",
    "evaluates to an array of probabilities of the entries in `values`, determined by the Poisson $(1)$ formula above.\n",
    "\n",
    "The acronym `pmf` stands for \"probability mass function\". It is common for probabilists to think of the probabilities in the distribution of a discrete random variable as masses attached to the random variable's possible values. \n",
    "\n",
    "The textbook has a useful [review](http://prob140.org/textbook/content/Chapter_07/00_Poissonization.html#an-interpretation-of-the-parameter) of how to plot probability histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 2a) The Poisson $(1)$ Distribution ###\n",
    "Draw a histogram of the Poisson $(1)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "k = np.arange(11)        # selected possible values\n",
    "poisson_1_probs = ...    # array of corresponding Poisson (1) probabilities\n",
    "poisson_1_dist = ...\n",
    "Plot(poisson_1_dist)\n",
    "plt.title('Poisson (1) Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Notice:\n",
    "\n",
    "- The distribution has two modes, at 0 and 1. In exercises you will see why.\n",
    "- Though there are infinitely many possible values, the set of *probable* values is very small — there is hardly any probability visible beyond the value 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Roughly stated, a theorem we proved in class says that if $n$ is large and $p$ is small, then the binomial $(n, p)$ probabilities are close to Poisson $(np)$ probabilities. \n",
    "\n",
    "Therefore, for large $n$ binomial $(n, 1/n)$ probabilities should be close to Poisson $(1)$ probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 2b) Binomial $(10, 1/10)$ Distribution ###\n",
    "If `values` is an array of integers in the range 0 through $n$, then\n",
    "\n",
    "`stats.binom.pmf(values, n, p)`\n",
    "\n",
    "evaluates to an array of the binomial $(n, p)$ probabilities of the entries in `values`.\n",
    "\n",
    "Display a histogram of the binomial $(10, 1/10)$ distribution. Notice how the probabilities are concentrated on the low values. This is a signal to start thinking about Poisson approximations, even though the number of trials ($n = 10$) isn't very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "# binomial (10, 1/10) distribution\n",
    "\n",
    "k = ...                            # possible values\n",
    "binom_probs = ...                  # binomial (10, 1/10) probabilities\n",
    "\n",
    "\n",
    "binom_dist = ...\n",
    "\n",
    "Plot(binom_dist)\n",
    "plt.title('Binomial (10, 1/10) Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Since $n = 10$ isn't very large, this distribution doesn't look exactly like the Poisson $(1)$ distribution though the two have many similarities. Run the cell below to see the overlaid histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "outputs": [],
   "source": [
    "k = np.arange(11)\n",
    "poisson_1_probs = stats.poisson.pmf(k, 1)\n",
    "poisson_1_dist = Table().values(k).probabilities(poisson_1_probs)\n",
    "\n",
    "Plots('Binomial (10, 1/10)', binom_dist, 'Poisson (1)', poisson_1_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 2c) Binomial $(100, 1/100)$ Distribution ###\n",
    "The Poisson $(1)$ approximation gets better as $n$ increases. To see this, draw overlaid histograms of the binomial $(100, 1/100)$ distribution and the Poisson $(1)$ distribution. \n",
    "\n",
    "You can use as many lines as you need for the code. As you can see from the histograms above, it's quite safe to draw the distributions over only the values $0$ through $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "outputs": [],
   "source": [
    "poisson_1_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Part 3: TVD between the Binomial $(n, 1/n)$ Distribution and its Poisson Approximation ##\n",
    "\n",
    "You can now quantify how good the Poisson approximation is.\n",
    "\n",
    "### 3a) Computing the TVD ###\n",
    "Define a function `binomial_Poisson_tvd` that takes $n$ as its argument and returns the total variation distance between the binomial $(n, 1/n)$ and Poisson $(1)$ distributions. \n",
    "\n",
    "Use as many lines of code as you need. It's fine to compute the Poisson $(1)$ probabilities only on the values $0$ through $n$, which are the possible values of the binomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "def binomial_Poisson_tvd(n):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "As a check to see if your function is working correctly, run the cell below. The output should be about $1$%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "binomial_Poisson_tvd(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 3b) Error in Approximation ###\n",
    "Now see what happens as $n$ gets large. Extend `tvd_table` defined below with a column labeled `'Binomial (n, 1/n)'` that contains the TVD between the binomial $(n, 1/n)$ and Poisson $(1)$ distributions, where in each row $n$ is given by the entry in Column 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "tvd_table = Table().with_columns('n', np.arange(5, 101))\n",
    "\n",
    "# array of tvds\n",
    "bin_Poi_tvds = tvd_table...\n",
    "\n",
    "tvd_table = tvd_table...\n",
    "\n",
    "tvd_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Run the cell below to plot the TVD as a function of $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Fill in the blanks (use the code cell below if you need to):\n",
    "\n",
    "For values of $n$ that are about $\\underline{~~~~~~~~~~~~~~~~}$ or more, Poisson $(1)$ approximations to binomial $(n, 1/n)$ probabilities will be off by at most 0.5%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "source": [
    "\n",
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "Now you can use total variation distance to help answer the question, \"How large does $n$ have to be before I can use the Poisson $(1)$ approximation to the binomial $(n, 1/n)$ distribution?\" \n",
    "\n",
    "- First decide how much error you are prepared to tolerate in your approximations. \n",
    "- Then use `tvd_table` (or an extended one with larger values of $n$) to find the smallest $n$ for which the TVD is below your threshold error. \n",
    "- For that $n$ or larger, the error in your Poisson $(1)$ approximations to binomial $(n, 1/n)$ probabilities will be below your threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "# Part A of the lab ends here, and is due by 11:59 pm Monday September 14 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Part 4: A Simple Bound for the TVD ###\n",
    "\n",
    "You have calculated the TVD by calculating both the exact binomial and the approximate Poisson probabilities, and you have seen that the TVD is small for large $n$.\n",
    "\n",
    "But what's the point of approximations and TVD if we have to compute the exact binomial probabilities to get the TVD in the first place?\n",
    "\n",
    "In fact there is no point in approximation if you can just find the exact chance. But for large enough $n$, the computations become infeasible. So probabilists have worked out simple mathematical upper bounds on the TVD between the Poisson and binomial (or binomial-like) distributions. The fundamental work in this area was by the late [Prof. Lucien LeCam](https://en.wikipedia.org/wiki/Lucien_Le_Cam) of the Statistics department at Berkeley.\n",
    "\n",
    "### 4a) [Written] LeCam's Theorem ###\n",
    "\n",
    "Read the statement of [LeCam's Theorem](https://en.wikipedia.org/wiki/Le_Cam%27s_theorem). It is about the Poisson approximation to the *Poisson-binomial* distribution. That's the distribution of the sum of independent indicators that need not be identically distributed.\n",
    "\n",
    "First explain why the binomial distribution is a special case of the Poisson binomial distribution.\n",
    "\n",
    "Then figure out how LeCam's theorem applies to the approximation you are studying in this lab, and hence find LeCam's upper bound on the TVD between the binomial $(n, 1/n)$ distribution and its Poisson $(1)$ approximation.\n",
    "\n",
    "You just have to apply the theorem carefully. Think about what the $p_i$'s have to be, and be careful about factors of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 4b) Confirming the Upper Bound ###\n",
    "The proof of LeCam's theorem is beyond the scope of this course. But you can check that the theorem does indeed give an upper bound, by drawing overlaid plots of the TVD and the bound.\n",
    "\n",
    "Complete the cell below to augment `tvd_table` with a column `\"LeCam's Bound\"` that contains the bound you found in **a**, and draw the overlaid plots. \n",
    "\n",
    "The limits on the vertical axis have been chosen for comparability with the plot in Part **3b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "outputs": [],
   "source": [
    "tvd_table = tvd_table.with_columns(\n",
    "    \"LeCam's Bound\", ...\n",
    ")\n",
    "\n",
    "tvd_table.plot(0)\n",
    "plt.ylim(0, 0.06);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "### 4c) Using the Bound ###\n",
    "\n",
    "Fill in the blank without calculation. \n",
    "\n",
    "If you use the Poisson $(1)$ distribution to approximate binomial $(1000, 1/1000)$ probabilities, the worst error you can make is no more than $\\underline{~~~~~~~~~~~~~~~~~~~~}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": true
   },
   "source": [
    "\n",
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "#newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Part 5: Biggest Possible Error ##\n",
    "\n",
    "To conclude the lab, you are going to prove that the TVD can be interpreted as a maximum difference of probabilities.\n",
    "\n",
    "We are going to compare two probability distributions $P_{blue}$ and $P_{gold}$ on a finite set of values $S$. Suppose the values are labeled $1, 2, \\ldots, n$. \n",
    "\n",
    "The *total variation distance between $P_{blue}$ and $P_{gold}$* is defined as\n",
    "\n",
    "$$\n",
    "\\| P_{blue} - P_{gold}\\|_{TVD} ~ = ~\n",
    "\\max\\{ \\lvert P_{blue}(A) - P_{gold}(A) \\rvert : A \\subseteq S\\}\n",
    "$$\n",
    "\n",
    "The definition says: For every event $A$, compute how far off $P_{blue}(A)$ is from $P_{gold}(A)$. The TVD is the biggest value among all these differences.\n",
    "\n",
    "That doesn't look at all like what we have been calculating as the TVD starting way back in Data 8. But in fact it's the same thing. It's your job to show how. \n",
    "\n",
    "Before you get started, confirm your understanding of the definition. Suppose you calculate the TVD between two distributions and get 0.003. That says that if you list all possible events and compare their probabilities under the two distributions, the biggest difference you will get is 3/1000. The two distributions are pretty close. \n",
    "\n",
    "The goal of this part of the lab is to show that this new definition of TVD is equivalent to the calculation we have been doing all along. Let's start by setting up some notation. For each $i$ in $S$, let $P_{blue}(i) = b_i$ and let $P_{gold}(i) = g_i$. If you imagine a bar graph or histogram of each distribution, then $b_i$ is the area of the blue bar at the value $i$, and $g_i$ is the area of the gold bar at $i$.\n",
    "\n",
    "In this notation, our familiar calculation of the TVD is\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i \\in S} \\lvert b_i - g_i \\rvert\n",
    "$$\n",
    "\n",
    "In this question and the next you will show that \n",
    "\n",
    "$$\n",
    "\\max\\{ \\lvert P_{blue}(A) - P_{gold}(A) \\rvert : A \\subseteq S\\} ~ = ~ \n",
    "\\frac{1}{2} \\sum_{i \\in S} \\lvert b_i - g_i \\rvert\n",
    "$$\n",
    "\n",
    "Three events will be important in the calculations.\n",
    "\n",
    "The set of values for which the blue bars exceed the gold:\n",
    "$$\n",
    "B = \\{i: b_i > g_i\\}\n",
    "$$\n",
    "\n",
    "The set of values for which the gold bars exceed the blue:\n",
    "$$\n",
    "G = \\{i: g_i > b_i\\}\n",
    "$$\n",
    "\n",
    "The set of values for which the blue bars and gold bars are equal:\n",
    "$$\n",
    "E = \\{i: b_i = g_i\\}\n",
    "$$\n",
    "\n",
    "Keep in mind that for any event $A$,\n",
    "$$\n",
    "P_{blue}(A) = \\sum_{i \\in A} b_i ~~~~~~~ \\text{and} ~~~~~~~\n",
    "P_{gold}(A) = \\sum_{i \\in A} g_i\n",
    "$$\n",
    "\n",
    "### This part of the lab is entirely written. There is no code. ###\n",
    "\n",
    "### 5a) ###\n",
    "Find the value of\n",
    "$$\n",
    "\\sum_{i \\in B} b_i ~ + ~ \n",
    "\\sum_{i \\in G} b_i ~ + ~ \n",
    "\\sum_{i \\in E} b_i \n",
    "$$\n",
    "\n",
    "Repeat the calculation after replacing $b_i$ by $g_i$ in all three sums above.\n",
    "\n",
    "Hence show that\n",
    "$$\n",
    "\\sum_{i \\in B} (b_i - g_i) ~ = ~ \\sum_{i \\in G} (g_i - b_i)\n",
    "$$\n",
    "\n",
    "This proves a statement we have made in Data 8 and Lab 2: \"The amount by which the blue bars exceed the gold is the same as the amount by which the gold bars exceed the blue.\" \n",
    "\n",
    "### 5b) ###\n",
    "Our usual calculation of TVD is\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i \\in S} \\lvert b_i - g_i \\rvert\n",
    "$$\n",
    "\n",
    "Partition the sum into two pieces to show that\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i \\in S} \\lvert b_i - g_i \\rvert ~ = ~\n",
    "\\sum_{i \\in B} (b_i - g_i) ~ = ~ \\sum_{i \\in G} (g_i - b_i)\n",
    "$$\n",
    "\n",
    "This proves another statement we made in Data 8 and Lab 2: \"The TVD is the amount by which the blue bars exceed the gold.\"\n",
    "\n",
    "### 5c) ###\n",
    "Now let $A$ be any event. Show that \n",
    "$$\n",
    "P_{blue}(A) - P_{gold}(A) ~ = ~ \n",
    "\\sum_{i \\in AB} (b_i - g_i) ~ - ~ \\sum_{i \\in AG} (g_i - b_i)\n",
    "$$\n",
    "\n",
    "Hence show that\n",
    "$$\n",
    "P_{blue}(A) - P_{gold}(A) ~ \\le ~ \n",
    "\\sum_{i \\in AB} (b_i - g_i) ~~~~~~ \\text{and} ~~~~~~\n",
    "P_{gold}(A) - P_{blue}(A) ~ \\le ~ \n",
    "\\sum_{i \\in AG} (g_i - b_i)\n",
    "$$\n",
    "\n",
    "### 5d) ###\n",
    "Use the first of the two inequalities in **c** to show that if $P_{blue}(A) - P_{gold}(A) > 0$ then\n",
    "\n",
    "$$\n",
    "\\lvert P_{blue}(A) - P_{gold}(A) \\rvert ~ \\le ~ \\sum_{i \\in B} (b_i - g_i)\n",
    "$$\n",
    "\n",
    "Use the second of the two inequalities in **c** to show that if $P_{blue}(A) - P_{gold}(A) < 0$ then\n",
    "\n",
    "$$\n",
    "\\lvert P_{blue}(A) - P_{gold}(A) \\rvert ~ \\le ~ \\sum_{i \\in G} (g_i - b_i)\n",
    "$$\n",
    "\n",
    "### 5e) ###\n",
    "Identify an event for which one of the inequalities in **d** is an equality.\n",
    "\n",
    "Explain why you now have a complete proof of\n",
    "\n",
    "$$\n",
    "\\max\\{ \\lvert P_{blue}(A) - P_{gold}(A) \\rvert : A \\subseteq S\\} ~ = ~ \n",
    "\\frac{1}{2} \\sum_{i \\in S} \\lvert b_i - g_i \\rvert\n",
    "$$\n",
    "\n",
    "That is, our usual calculation of the TVD is equivalent to finding the biggest difference between probabilities assigned by the two distributions to any event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Conclusion ##\n",
    "What you have learned in this lab:\n",
    "- Many random variables in this lab have a large number of possible values, and the approximating Poisson distribution has infinitely many possible values. But no matter how large $n$ is, the *probable* values of all the variables are in a very small range — 0 through about 8 or 10 — because all of the distributions are roughly Poisson $(1)$.\n",
    "- If you use an approximation to the distribution of $X$, then the total variation distance between the exact and approximate distributions measures the worst error you can make in approximating probabilities of events determined by $X$. \n",
    "- The total variation distance between the binomial $(n, 1/n)$ and Poisson $(1)$ distributions falls sharply as a function of $n$ and is below 1% even for moderate values of $n$.\n",
    "- There is a simple upper bound for this total variation distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "#solution": false,
    "#staff": false,
    "#student": false
   },
   "source": [
    "## Submission Instructions ##\n",
    "\n",
    "Many assignments throughout the course will have a written portion and a code portion. Please follow the directions below to properly submit both portions.\n",
    "\n",
    "### Written Portion ###\n",
    "\n",
    "- Scan all the pages into a PDF. You can use any scanner or mobile application. There are many free apps available that allow you to convert your work into PDFs from your phone. Please DO NOT simply take pictures using your phone.\n",
    "- Please start a new page for each question. If you have already written multiple questions on the same page, you can crop the image or fold your page over (the old-fashioned way). This helps expedite grading.\n",
    "- It is your responsibility to check that all the work on all the scanned pages is legible.\n",
    "\n",
    "### Code Portion ###\n",
    "\n",
    "- Save your notebook using File > Save and Checkpoint.\n",
    "- Generate a PDF file using File > Download as > PDF via LaTeX. This might take a few seconds and will automatically download a PDF version of this notebook.\n",
    "    - If you have issues, please make a follow-up post on the general Lab 1 Piazza thread.\n",
    "\n",
    "### Submitting ###\n",
    "\n",
    "- Combine the PDFs from the written and code portions into one PDF. [Here](https://smallpdf.com/merge-pdf) is a useful tool for doing so.\n",
    "- Submit the assignment to Lab 1 on Gradescope.\n",
    "- **Make sure to assign each page of your pdf to the correct question.**\n",
    "- It is your responsibility to verify that all of your work shows up in your final PDF submission.\n",
    "- If you have questions about scanning or uploading your work, please post a follow-up to the [Piazza thread](https://piazza.com/class/kdz50ztjhsq3jf?cid=23) on this topic."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "checksums": [
   "f55a7e1c90e2595b233c3956a2783786",
   "9c78703d0d09b2932d35ae75053a5129",
   "bbbd0bf658428e10cbfd889247879924",
   "b13dc13b0b8e8b24b97b329a0ee83b8d",
   "518c84f8a1f148dbd174743a2ce02a06",
   "215ed6c8654224bcdb429ca58d5e7135",
   "4fb206b23cca8f1941265599b0f0f320",
   "91ecad7bcd30fea85846698c6c4ad236",
   "b182444b425e530aebac0b1429b8d999",
   "c58db66b6fee81063dd7637932e6e637",
   "8d31345e276966579a2701d3c436204e",
   "dc8bbeae78e29542dbc5821cbfe3e544",
   "05b6c4f7180c1a2613cba1b9916e62c6",
   "ca211c8d334df1876ebb8c81418793a5",
   "3d520b5d8413062b12e215ec2931eebd",
   "640061103a186fd26f59746dd4d6de57",
   "5646fedb6c11438b6bb6dd3dd812d898",
   "4d9b5f681cd66d3d4414fd2304a3bdca",
   "f3591ed5365d1d6c21c152b342e43c91",
   "17c82d80cd605f89249c1a4dcaa6cf5d",
   "081425877328bf50a59b899def08cc38",
   "d7c8e6530fa586d441e79b630e5d7ec3",
   "59d1811a313c25ca19581e2a23867f7e",
   "205086c284ede8226b620c7e3a2f8aaf",
   "1ba5798afe8b4b9b50a92451638e4938",
   "d7108123f64f75aaaf9ee6f7fb3957f2",
   "17432c855ee16c8b620c71065f455c34",
   "2f43b42fd833d1e77420a8dae7419000",
   "df2f38a828bb8d0d8c558c32fff0bfd5",
   "7a5cd28139a9680b5f6c5cba3b285b3c",
   "71f2f1609b5cc73741f9e07b05ab0e6c",
   "732477c7f5d4080670738ff6df6da214",
   "44fd51da968ba3e9826a7bfc01c5315b",
   "921bb7da7d102dcec095383a7e2be099",
   "858880ddcf3708c55f0adb1fd107ee7d",
   "76f010a283b70981ae95c2a275fc9b73",
   "7349b9cd2618e0f9c89b53d2feedcacc",
   "0de20b93bb878d597e59213991465648",
   "2f43b42fd833d1e77420a8dae7419000",
   "06edb87aa884a26defc24dd58f8fd9b9",
   "fa2f1ac5d9c2376ff6f9b742bb134c22",
   "640061103a186fd26f59746dd4d6de57",
   "e0a2056f43ccb01c1a160ee30ae49f1a",
   "4c721ccde94d2a19f909162b14194484",
   "90d2492e6af7a7fa061f94b6f2806a82",
   "eb94ad42085659b497d3261c55a205f1",
   "0de20b93bb878d597e59213991465648",
   "640061103a186fd26f59746dd4d6de57",
   "3aab812967a6e75478f882c1abcfbf57",
   "b56d16fd0b2f8d1315d12d9a89e332d2",
   "808f6d4f4212e98ffcb0e12bca6537b5"
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "number_of_pagebreaks": 3,
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
